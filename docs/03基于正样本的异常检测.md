## 基于无监督学习的工业图像异常检测

# 一、前置

## 1、异常检测定义

如果将正常样本看作空间中的一个点的话，异常（anomaly）样本简单地来说就是离群点，或者说是“非主流”的点，即离大多数样本点都比较远。异常通常是少数。

- 检测领域：3C、半导体、汽车、食品和纺织制造、医药、军工、新能源等；

- 工业场景中常见的缺陷检测：

​		1、表面外观缺陷：杂质异物，划痕，脏污，破损，褶皱，色彩，印刷不良等；

​		2、结构缺陷：位置，安装漏、反，漏印刷，与设计图纸不匹配等；

​		3、其他缺陷：肉眼不能对应，需要特别设计的光学或图像处理，例如医学图像，红外热呈像，超声波呈像，卫星图像等；

- 常用的检测方案：

​		1、传统的数字图像处理：依据缺陷颜色、形状等特征。适用于简单规则的工业场景。缺点：配置参数复杂；对特征的表达能力差；对于复杂场景难以处理。

​		2、深度学习方法：将已有的成熟的通用目标的分类、检测和分割模型应用于工业场景，根据实际需求调整和改进。优点：相比于传统数字图像处理算法，对特征的表达能力更强，能够处理复杂的工业图像数据，且性能又是好，在很多场景下逐渐取代传统算法方案。

## 2、工业图像数据特点和挑战

- **挑战：**

1、异常样本难获取：发生率低；保留时间段短（定期清理/原材料回收返工）；造缺陷样本成本高，且与真实缺陷发生形态不能拟合；

2、异常类的异构性：不同类的形态一致或者差距小，对于不同异常分类有挑战；同一类别的形态特征分布多样性高，特征难以拟合；

3、类别不平衡：异常数据呈长尾型分布；

4、标注工作量大：常用的有监督学习方案，如目标检测、分割等太依赖于标注的数据；

5、指标高：动辄100%， 0漏检；

- **特点：**

1、尺度变化小，位置相对固定；

2、光照稳定，特定设备、空间中的相机 + 光源设计；

3、背景一致性高；

4、产品规格类别相对较少（对于规格多、且每种规格形态、纹理、颜色、位置等不固定不适用）；

# 二、无监督学习检测缺陷

面对工业图像数据的特点和挑战，借鉴异常检测的思路对易于获取的正常样本建模检测工业图像中的缺陷。

```markdown
**I.基于重构的方法**
A.AutoEncoder
B.GAN
**II.基于表示的方法**
A.PatchCore
B.FastFlow
```

## 1、基于重构的方法

利用生成模型，如自动编码器或者生成对抗网络（GAN）来编码和重构正常数据（这些方法认为异常是不能被重建的，因为他们不存在于训练样本）。

### 1-1、AutoEncoder(自动编码器)

编码器将原始数据映射到低维特征空间，而解码器试图从投影的低维空间恢复数据。通过重构损失函数学习。

![image-20231024163215514](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024163215514.png)

![image-20231024163222545](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024163222545.png)

**将自动编码器用于异常检测是基于正常实例比异常实例能从压缩的特征空间更好地被重构这一假设。**

改进：

- Denoising AutoEncoder：

![image-20231024163340259](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024163340259.png)

- VAE（变分自编码器）

可变自动编码器以概率方式(分布)编码输入的潜在属性，而不是像普通的自动编码器那样以确定性方式(单值)编码。

![image-20231024163535265](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024163535265.png)

### 1-2、GAN(生成对抗模型)

训练：可以通过网络仅仅学习正常数据的数据分布，得到的模型G只能生成或者重建正常数据。

测试：使用测试样本输入训练好的模型G，如果G经过重建后输出和输入一样或者接近，表明测试的是正常数据，否则是异常数据。

典型案例：AnoGAN、GANomaly、Wasserstein GAN、 Cycle GAN等

**将生成模型用于异常检测是基于在生成网络的潜在特征空间中正常实例比异常实例能够更准确地被产生这一假设。**

![image-20231024164931446](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024164931446.png)

推断异常方式：使用第一部分编码器产生的隐空间特征（原图的编码）和第二部分编码器产生的隐空间特征（重建图的编码）的差异。 可改进自编码器中易受噪声影响的问题，鲁棒性更好。

## 2、基于表示的方法

### 2-1、PatchCore

使用深度卷积神经网络提取正常图像或者正常图像块的判别特征，并建立这些特征的分布。这些方法通过计算测试图像的特征与正常特征分布之间的距离来获得异常得分。

PatchCore: Towards Total Recall in Industrial Anomaly Detection --https://arxiv.org/pdf/2106.08265v2.pdf

![image-20231024165218026](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165218026.png)

​																				PatchCore模型架构

**Pretrained Encoder**: 使⽤预训练模型（wide_resnet50_2） backbone 提取图像特征, 采⽤ [2, 3] 层特征作为图像特征。

**Locally aware patch** **features**：提取图像的 Patch特征，这个特征带有周围数据的信息。特征值的集合构建 PatchCore memory bank。

- Patch-feature 相对于直接对Patch的马氏距离度量（PaDiM），对图像的依赖性降低；

- 特征提取：特征表⽰为何不选择⽹络特征层次的最后⼀级：（1）会丢失更多的局部正常样本信息；（2）深层特征具有更强的语义信息，偏向于分类任务。

- patch特征：可以理解为 训练图⽚上所有的点，以该点为中⼼的邻居点集得到的特征值，特征值的集合就是 PatchCore memory bank。

   以( h , w ) 为中⼼，以p 为直径的正⽅形包围住的点：

![image-20231024165341210](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165341210.png)

围绕这些点计算的特征图上的点为：

![image-20231024165357989](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165357989.png)

⼀张图像的Patch特征集合：

![image-20231024165506031](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165506031.png)

正常训练集图像的Patch特征集合：

![image-20231024165517383](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165517383.png)

**Coreset-reduced patch-feature memory bank**：Coreset Subsampling 核⼼集⼆次抽样，稀疏采样⽬的是Reduce memory bank，加快算法运⾏速度。含义为找到一个特征集合 Mc，使得 M 中任意特征  距离 Mc 中最近的特征的距离的最大值最小。

![image-20231024165551693](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165551693.png)

- **Anomaly Detection(Inference)** ：

![image-20231024165822908](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165822908.png)

提取Patch特征：

![image-20231024165831891](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024165831891.png)

对于每个 Patch 特征选择 Mc 中距离最近的特征 m*; 

计算异常值分数：集合P(X^test) 到M的距离。公式arg min ||m^test - m|| 表⽰点m^test到集合M的距离，然后找到最远的点m^test,*;  选择距离最远的异常值， 距离最远的点为图像的异常值分数。

![image-20231024170459573](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024170459573.png)

文中强调为了提升算法鲁棒性，采用一个公式更新了异常值：

![image-20231024170626204](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024170626204.png)

我的理解为：对于特征系数的区域倾向于判定为异常 （在特征系数的区域内 ，分子较小，异常值会更大），反正给予异常值一定的削减（否则，分子/分母 会更大，异常值会减弱。）每个点的异常值拼接起来即可获得图像的异常热力图。

### 2-2、FastFlow

**FastFlow**: 基于表⽰的⽅法，从vision transformer 或者 resnet 中提取视觉特征，并通过FastFlow模型建⽴分布。

![image-20231024174001252](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024174001252.png)

​																						FastFlow模型结构

FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows -- https://arxiv.org/pdf/2111.07677v2.pdf

- 1、特征提取模块：backbone 充当特征提取器，不需要训练，不需要更新参数 requires_grad==false。

- 2、norms 模块：对于 resnet，norms 是可训练的 LayerNorm。

- 3、flows模块：可逆神经⽹络。(参考FrEIA)

![image-20231024174802962](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024174802962.png)

- 4、损失函数：负对数似然

![image-20231024174817298](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024174817298.png)

- 指标对比

![image-20231024175104314](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20231024175104314.png)



# 三、Update

## 1、CPR

主要思想：给定测试样本，首先根据鲁棒的直方图匹配过程选择前 K 个最相似的训练图像。其次，通过使用仔细训练的局部度量，在这些“全局最近邻居”的相似几何位置上检索每个测试补丁的最近邻居。最后，根据到其“局部最近邻”的距离和“非背景”概率计算每个测试图像块的异常分数。



## 2、EfficientAD

《EfficientAD.md》



## 3、RegAD

《RegAD.md》



## 4、PNI : Industrial Anomaly Detection using Position and Neighborhood Information

```python
由于异常样本不能用于训练，因此许多异常检测和定位方法使用预训练网络和非参数建模来估计编码特征分布。然而，这些方法忽略了位置和邻域信息对法线特征分布的影响。为了克服这个问题，我们提出了一种新算法 \textbf{PNI}，它使用给定邻域特征的条件概率来估计正态分布，并用多层感知器网络建模。此外，通过创建每个位置处的代表性特征的直方图来利用位置信息。所提出的方法不是简单地调整异常图的大小，而是采用在合成异常图像上训练的附加细化网络，以更好地插值和解释输入图像的形状和边缘。
```



## 5、SimpleNet: A Simple Network for Image Anomaly Detection and Localization

```python
我们提出了一个简单且应用程序友好的网络（称为 SimpleNet）来检测和定位异常。 SimpleNet 由四个组件组成：（1）一个生成局部特征的预训练特征提取器，（2）一个将局部特征转移到目标域的浅层特征适配器，（3）一个简单的异常特征生成器，通过添加高斯来伪造异常特征正常特征的噪声，以及（4）区分异常特征和正常特征的二元异常鉴别器。在推理过程中，异常特征生成器将被丢弃。我们的方法基于三个直觉。首先，将预先训练的特征转换为面向目标的特征有助于避免领域偏差。其次，在特征空间中生成合成异常更为有效，因为缺陷在图像空间中可能没有太多共性。第三，简单的判别器非常高效且实用。尽管简单，SimpleNet 在数量和质量上都优于以前的方法。在 MVTec AD 基准上，SimpleNet 实现了 99.6% 的异常检测 AUROC，与排名第二的最佳性能模型相比，误差减少了 55.5%。此外，SimpleNet 比现有方法更快，在 3080ti GPU 上具有 77 FPS 的高帧速率。此外，SimpleNet 在单类新颖性检测任务上表现出显着的性能改进。代码：https://github.com/DonaldRR/SimpleNet。
```



## 6、PaDiM

《PaDiM.md》



## 7、ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection

```python
异常检测对于工业制造中产品缺陷（例如不正确的零件、未对准的组件和损坏）的高级识别至关重要。由于观察很少且缺陷类型未知，异常检测被认为在机器学习中具有挑战性。为了克服这一困难，最近的方法利用自然图像数据集中的常见视觉表示并提取相关特征。然而，现有的方法仍然存在预训练特征和目标数据之间的差异，或者需要输入增强，而输入增强应该特别针对工业数据集进行仔细设计。在本文中，我们介绍了 ReConPatch，它通过训练附加到预训练模型的线性调制来构建用于异常检测的判别特征。 ReConPatch 采用对比表示学习来收集和分配特征，从而产生面向目标且易于分离的表示。为了解决对比学习中缺少标记对的问题，我们利用数据表示之间的两种相似性度量，即成对相似性和上下文相似性作为伪标签。与之前的工作不同，ReConPatch 无需大量输入增强即可实现强大的异常检测性能。我们的方法为广泛使用且具有挑战性的 MVTec AD 数据集实现了最先进的异常检测性能 (99.72%)
```



## 8、MSFlow: Multi-Scale Flow-based Framework for Unsupervised Anomaly Detection

```python
无监督异常检测（UAD）吸引了很多研究兴趣并推动了广泛的应用，其中只有无异常样本可用于训练。一些UAD应用程序打算在没有任何异常信息的情况下进一步定位异常区域。尽管缺乏异常样本和注释会降低 UAD 性能，但一个不起眼但功能强大的统计模型（归一化流）适合以无监督方式进行异常检测和定位。基于流的概率模型仅在无异常数据上进行训练，可以通过为不可预测的异常分配比正常数据低得多的可能性来有效地区分它们。然而，不可预测的异常的大小变化给基于流的高精度异常检测和定位方法带来了另一个不便。为了概括异常大小的变化，我们提出了一种新颖的基于多尺度流的框架，称为 MSFlow，由不对称并行流和随后的融合流组成，以交换多尺度感知。此外，根据图像异常检测和像素异常定位之间的差异，采用不同的多尺度聚合策略。所提出的 MSFlow 在三个异常检测数据集上进行了评估，显着优于现有方法。值得注意的是，在具有挑战性的 MVTec AD 基准上，我们的 MSFlow 达到了新的最先进水平，检测 AUORC 分数高达 99.7%，定位 AUCROC 分数为 98.8%，PRO 分数为 97.1%。可重现的代码可在 https://github.com/cool-xuan/msflow 获取
```



## 9、DiffusionAD: Norm-guided One-step Denoising Diffusion for Anomaly Detection

```python
DiffusionAD 是一种用于异常检测和定位的新颖框架，由重建子网络和分割子网络组成。重建子网络通过扩散模型实现，其任务是将异常图像恢复为无异常图像。分割子网络使用输入图像及其无异常恢复来预测像素级异常分数。值得注意的是，我们采用了一步去噪范式，这比迭代去噪方法要快得多。此外，所提出的规范引导范式增强了无异常重建的保真度。
```







# 最后的倔强：

- 对于规格多、且每种规格形态、纹理、颜色、位置等不固定情况如何优化？

Registration based Few-Shot Anomaly Detection -- https://arxiv.org/pdf/2207.07361v1.pdf



- 数据脏的情况：不同缺陷不同检测标准（对产品影响的严重等级），检出的缺陷标准、界限难定；



- 单场景如何快速规模复制；单场景扩展到多个不同场景。
  - 领域内大模型？
  - 垂直行业成熟、通用性高的算法库？



- 如何加快开发效率、控制开发成本？
  - 团队通用的算法库，并逐渐积累
  - 一致的开发环境、部署环境
  - 高效的系统展示平台，易于改动、对接且效率高。





# 复现记录：

## 1、PatchCore

### EX1

- 参数：

```python
def get_args():
    import argparse
    parser = argparse.ArgumentParser(description='ANOMALYDETECTION')
    parser.add_argument('--phase', choices=['train','test'], default='train')
    parser.add_argument('--dataset_path', default=r'/root/dataset/public/mvtec_anomaly_detection')
    parser.add_argument('--category', default='bottle')
    # num_epochs: patchCore 没有 PaDiM 那样的训练阶段（神经网络）,在代码中它只是提取特征而不更新参数。epochs=1
    parser.add_argument('--num_epochs', default=1)
    parser.add_argument('--batch_size', default=16)
    parser.add_argument('--load_size', default=256)
    parser.add_argument('--input_size', default=224)
    # coreset_sampling_ratio 
    parser.add_argument('--coreset_sampling_ratio', default=0.1)
    parser.add_argument('--project_root_path', default=r'/root/project/ad_algo/anomaly_detection/PatchCore_anomaly_detection/models/patchcore')
    parser.add_argument('--save_src_code', default=True)
    parser.add_argument('--save_anomaly_map', default=True)
    parser.add_argument('--n_neighbors', type=int, default=9)
    args = parser.parse_args()
    return args
```

- result:

```python
# python train_wood.py 
pip install faiss
pip install faiss-gpu==1.7.1.post3

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│          img_auc          │            1.0            │
│         pixel_auc         │    0.9814527156078976   
```



### EX2：

```python
def get_args():
    import argparse
    parser = argparse.ArgumentParser(description='ANOMALYDETECTION')
    parser.add_argument('--phase', choices=['train','test'], default='train')
    parser.add_argument('--dataset_path', default=r'/root/dataset/public/GM06_08')
    parser.add_argument('--category', default='GM06_08')
    # num_epochs: patchCore 没有 PaDiM 那样的训练阶段（神经网络）,在代码中它只是提取特征而不更新参数。epochs=1
    parser.add_argument('--num_epochs', default=1)
    parser.add_argument('--batch_size', default=32)
    parser.add_argument('--load_size', default=256)
    parser.add_argument('--input_size', default=224)
    # coreset_sampling_ratio 
    parser.add_argument('--coreset_sampling_ratio', default=0.01)
    parser.add_argument('--project_root_path', default=r'/root/project/ad_algo/anomaly_detection/PatchCore_anomaly_detection/models/patchcore')
    parser.add_argument('--save_src_code', default=True)
    parser.add_argument('--save_anomaly_map', default=True)
    parser.add_argument('--n_neighbors', type=int, default=9)
    args = parser.parse_args()
    return args
```

```python
python train_wood_customize.py
"""
└── GM06_08
    ├── test
    │   ├── good
    │   └── ng1
    └── train
        └── good
"""
result: 0.8929597701149425; 看效果图并不是很好，原图大小是1600（w）x1200(w)。 
==>减小bs, 增大分辨率 --load_size 1024 --input_size 1024 --batch_size 8。 python train_wood_customize.py; 
==> 在Coreset Subsampling 核心集二次抽样被杀死。将coreset_sampling_ratio更改为0.001， 失败。 
==> 将分辨率改为512,coreset_sampling_ratio=0.01, 核心集二次抽样要计算很久。test需要修改 anomaly_map = score_patches[:,0].reshape((64,64))
0.9935344827586207
```



## 2、FastFlow

```python
https://github.com/gathierry/FastFlow.git
https://github.com/vislearn/FrEIA
"""
# first clone the repository
git clone https://github.com/vislearn/FrEIA.git
cd FrEIA
# install the dependencies
pip install -r requirements.txt
# install in development mode, so that changes don't require a reinstall
python setup.py develop
"""
pip install -r requirements.txt
pip install pytorch-ignite==0.2.0
```

### EX1:

```python
def parse_args():
    parser = argparse.ArgumentParser(description="Train FastFlow on MVTec-AD dataset")
    # wide_resnet50_2.yaml resnet18.yaml
    parser.add_argument(
        "-cfg", "--config", default="/root/project/ad_algo/anomaly_detection/FastFlow/configs/resnet18.yaml", type=str, help="path to config file"
    )
    parser.add_argument("--data", default=r'/root/dataset/public/mvtec_anomaly_detection', type=str, help="path to mvtec folder")
    parser.add_argument(
        "-cat",
        "--category",
        default="bottle",
        type=str,
        choices=const.MVTEC_CATEGORIES,
        help="category name in mvtec",
    )
    parser.add_argument("--eval", action="store_true", help="run eval only")
    parser.add_argument(
        "-ckpt", "--checkpoint", type=str, help="path to load checkpoint"
    )
    args = parser.parse_args()
    return args
# constants.py NUM_EPOCHS = 50
```

- Result:

```python
AUROC: 0.9687134014718118

# eval
python main.py -cfg configs/resnet18.yaml --data /root/dataset/public/mvtec_anomaly_detection -cat bottle --eval -ckpt _fastflow_experiment_checkpoints/exp0/49.pt
```

### EX2:

```python
def parse_args():
    parser = argparse.ArgumentParser(description="Train FastFlow on MVTec-AD dataset")
    # wide_resnet50_2.yaml resnet18.yaml
    parser.add_argument(
        "-cfg", "--config", default="/root/project/ad_algo/anomaly_detection/FastFlow/configs/resnet18.yaml", type=str, help="path to config file"
    )
    parser.add_argument("--data", default=r'/root/dataset/public/bottle_in', type=str, help="path to data folder")
    parser.add_argument(
        "-cat",
        "--category",
        default="bottle_in",
        type=str,
        choices=const.MVTEC_CATEGORIES,
        help="category name in mvtec",
    )
    parser.add_argument("--eval", action="store_true", help="run eval only")
    parser.add_argument(
        "-ckpt", "--checkpoint", type=str, help="path to load checkpoint"
    )
    args = parser.parse_args()
    return args

```

- Result:

```python
AUROC: 1.0
# eval
python main_wood_customize.py -cfg configs/resnet18.yaml --data /root/dataset/public/bottle_in -cat bottle_in --eval -ckpt _fastflow_experiment_checkpoints/exp2/49.pt
```

## 3、EfficientAD

《EfficientAD.md》



# 四、基于 pytorch-lightning 的 anomalib

## 1、pytorch-lightning

《10_pytorch-lightning深度学习框架.md》

## 2、anomalib

### 2-1、环境依赖

```python
git clone https://github.com/openvinotoolkit/anomalib.git

cd anomalib
# 新工作：更新算法库依赖的python基础环境：Python3.7->python3.10
# pip install -e . 
pip install omegaconf
pip install jsonargparse 
pip install lightning==2.2.0.post0 # 1.9.5
pip install kornia==0.6.9
pip install torchmetrics==0.10.3
pip install rich-argparse
pip install einops
pip install freia
# from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
pip install mpmath
pip install open-clip-torch

# 指定单个GPU可训练，多个GPU当前存在问题
CUDA_VISIBLE_DEVICES=0 python train_test.py


```

更新算法库依赖的python基础环境：Python3.7->python3.10

```python
conda deactivate
conda create -n cv_env_310 python=3.10 -y

conda activate cv_env_310 && conda info --envs

# pip config set global.index-url pypi.mirrors.ustc.edu.cn
# pip config set install.trusted-host https://pypi.mirrors.ustc.edu.cn/simple/

pip install matplotlib==3.5.3
pip install bokeh==1.4.0
conda install nodejs
conda install ipykernel ipython \
    jupyter notebook
# 速度问题： -i https://pypi.tuna.tsinghua.edu.cn/simple
# 不能找到：可以使用以下命令将全局索引 URL 重置为默认值 
# pip config unset global.index-url

pip install opencv-python==4.7.0.72 opencv-contrib-python==4.7.0.72 sklearn==0.0

pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html 

pip install jupyter_contrib_nbextensions
pip install cython==0.29.23
pip install timm==0.6.13 
pip install pytorch-lightning # 2.2.0.post
pip install jinja2==3.0.1
pip install pandas
pip install h5py
pip install albumentations
# 先降低 pytorch-lightning==1.9.5 先测试原有算法库。
pip uninstall pytorch-lightning
pip install pytorch-lightning==1.9.5
pip install imgaug

/***
pip install torchsummary==1.5.1 fastai==1.0.61 timm==0.6.13 pytorch-lightning==1.9.5 onnxruntime-gpu

pip install fvcore==0.1.5.post20210812 \
    mmcv-full==1.3.3 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html

mkdir -p ~/public
cd ~/public 
git clone https://github.com/open-mmlab/mmdetection.git 
cd mmdetection && git checkout v2.13.0 && pip install -e . 
cd ~/public && git clone https://github.com/open-mmlab/mmsegmentation.git && \
cd mmsegmentation && git checkout v0.14.0 && pip install -e . && \
cd ~/public && git clone https://github.com/facebookresearch/detectron2.git && \
cd detectron2 && git checkout v0.5 && pip install -e .

rm /root/*.whl && \
rm -rf /root/conda/pkgs/* && \
rm -rf /root/.cache/pip/*
***/

jupyter-notebook password
nohup jupyter-notebook --no-browser --ip 0.0.0.0 --port 4350 --allow-root > jupyter.nohub.out.310 &
http://192.168.1.6:4350/tree?

# gtcv算法库中已有算法测试
cv库：c++库不能使用
分类：OK
目标检测：yolox
分割：OK
可视化相关：OK

# C++ 环境

# C ++ cython 编译
# add 测试：OK
python setup.py build_ext --inplace
# caliper 编译
python setup.py build_ext --inplace
#error
"""
/root/conda/envs/cv_env_310/compiler_compat/ld: cannot find -lopencv_features2d: 没有那个文件或目录
/root/conda/envs/cv_env_310/compiler_compat/ld: cannot find -lopencv_core: 没有那个文件或目录
/root/conda/envs/cv_env_310/compiler_compat/ld: cannot find -lopencv_imgcodecs: 没有那个文件或目录
/root/conda/envs/cv_env_310/compiler_compat/ld: cannot find -lopencv_imgproc: 没有那个文件或目录
/root/conda/envs/cv_env_310/compiler_compat/ld: cannot find -lopencv_dnn: 没有那个文件或目录
collect2: error: ld returned 1 exit status
error: command '/usr/bin/g++' failed with exit code 1
"""
# 在 /usr/local/lib下可找到对应的so文件
find ./ -name libopencv_features2d.so.406
# export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
python setup.py build_ext --inplace
# 依旧报错， 修改setup.py中 
# lib_dir = "./"
lib_dir = "/usr/local/lib/"

# matching编译， 同样，setup.py中 指定lib_dir = "/usr/local/lib/"
python setup.py build_ext --inplace

conda env remove --name cv_env_310
```

- C++ 环境：

```python
cp -r cmake-3.25.3.tar.gz  /path
cp -r eigen-3.3.9.tar.gz /path
cp -r opencv-4.6.0.zip /path
cp -r opencv_contrib-4.6.0.zip /path

# -------cmake -----------
apt update
apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3
apt-get install libgl1-mesa-dev
# https://cmake.org/download/
wget https://github.com/Kitware/CMake/releases/download/v3.25.3/cmake-3.25.3.tar.gz
tar -zxvf cmake-3.25.3.tar.gz
apt install gcc
apt install g++
apt install build-essential
apt install libssl-dev
cd cmake-3.25.3
./bootstrap
make -j8
make install



# -----------eigen---------
wget https://gitlab.com/libeigen/eigen/-/archive/3.3.9/eigen-3.3.9.tar.gz
tar -xzf eigen-3.3.9.tar.gz
cd eigen-3.3.9
mkdir build
cd build
# cmake ..  # 安装结果在 /usr/local/include/eigen3
# make
# make install 
# 安装在虚拟环境的include 下：
cmake .. -DCMAKE_INSTALL_PREFIX=/root/conda/envs/cv_env_310/include/eigen3
make
make install 


# ---------- opencv4.6.0------------
# 参考 https://blog.csdn.net/u014491932/article/details/124886394 
# 参考 https://blog.csdn.net/gentleman1358/article/details/126955032
# 参考 https://docs.opencv.org/4.x/d7/d9f/tutorial_linux_install.html
# https://github.com/opencv/opencv_contrib/releases/tag/4.6.0
apt-get install build-essential 

# libgtk2.0-dev 失败
apt-get install libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev

apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-22-dev

wget -O opencv-4.6.0.zip https://github.com/opencv/opencv/archive/refs/tags/4.6.0.zip
# 解压 opencv_contrib-4.6.0.zip 并放在opencv-4.6.0下
wget -O opencv_contrib-4.6.0.zip https://github.com/opencv/opencv_contrib/archive/refs/tags/4.6.0.zip
cp -r opencv_contrib-4.6.0 ./opencv-4.6.0
cd opencv-4.6.0

mkdir -p build && cd build
# sudo cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local ..
# cmake ..
cmake -DOPENCV_DOWNLOAD_URL=https://mirrors.tuna.tsinghua.edu.cn/opencv/ -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib-4.6.0/modules ..
make -j8
make install

# ln -s /usr/include/opencv4/opencv2 /usr/include/opencv2
# ln -s /usr/local/include/opencv4/opencv2 /usr/local/include/opencv2
# -- Installing: /usr/local/bin/opencv_version
# --.bashrc--
PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig
export PKG_CONFIG_PATH
export OpenCV_INCLUDE_DIRS=/usr/local/include/opencv4/opencv2
export EIGEN3_INCLUDE_DIR=/usr/local/include/eigen3

# lib 配置
vim /etc/ld.so.conf.d/opencv.conf 输入 /usr/local/lib 再退出 ldconfig

# 配置 /usr/local/lib/pkgconfig opencv.pc文件中加入
prefix=/usr/local
exec_prefix=${prefix}
includedir=${prefix}/include
libdir=${exec_prefix}/lib

Name: opencv
Description: The opencv library
Version:4.6.0
Cflags: -I${includedir}/opencv4
Libs: -L${libdir} -lopencv_shape -lopencv_stitching -lopencv_objdetect -lopencv_superres -lopencv_videostab -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_imgcodecs -lopencv_video -lopencv_photo -lopencv_ml -lopencv_imgproc -lopencv_flann  -lopencv_core


#vim /etc/bash.bashrc 文件最后添加 虚拟环境就添加再~/.bashrc即可
# PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig 
# export PKG_CONFIG_PATH
# source /etc/bash.bashrc

# 查看版本
pkg-config opencv --modversion

```

- 配置 .bashrc

```python
# 原来cv_env虚拟环境
# <<< conda initialize <<<
conda deactivate && conda activate cv_env
# conda deactivate && conda activate cv_env_310
PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig
export PKG_CONFIG_PATH
export OpenCV_INCLUDE_DIRS=/usr/local/include/opencv4/opencv2
export EIGEN3_INCLUDE_DIR=/usr/local/include/eigen3

# cv_env_310环境
conda deactivate && conda activate cv_env_310
PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig
export PKG_CONFIG_PATH
export OpenCV_INCLUDE_DIRS=/usr/local/include/opencv4/opencv2
export EIGEN3_INCLUDE_DIR=/usr/local/include/eigen3
```

### 2-2、算法集成

#### 2-2-1、直接集成anomalib模块

##### 1、数据格式：

- MvTec AD

```python
bottle
├── ground_truth
│   ├── broken_large
│   ├── broken_small
│   └── contamination
├── license.txt
├── readme.txt
├── test
│   ├── broken_large
│   ├── broken_small
│   ├── contamination
│   └── good
└── train
    └── good

```

- 自定义文件夹数据集 (Custom Folder Dataset.)

https://anomalib.readthedocs.io/en/latest/markdown/guides/reference/data/image/folder.html

```python
$ tree sample_dataset
sample_dataset
├── colour
│   ├── 00.jpg
│   ├── ...
│   └── x.jpg
├── crack
│   ├── 00.jpg
│   ├── ...
│   └── y.jpg
├── good
│   ├── ...
│   └── z.jpg
├── LICENSE
└── mask
    ├── colour
    │   ├── ...
    │   └── x.jpg
    └── crack
        ├── ...
        └── y.jpg
```

```python
folder_datamodule = Folder(
    root=dataset_root,
    normal_dir="good",
    abnormal_dir="crack",
    task=TaskType.SEGMENTATION,
    mask_dir=dataset_root / "mask" / "crack",
    image_size=256,
    normalization=InputNormalizationMethod.NONE,
)
folder_datamodule.setup()
```

- 文件夹数据集

假设我们想使用此文件夹数据集从文件夹创建数据集以进行分类任务。我们可以首先创建变换：

```python
from anomalib.data.utils import InputNormalizationMethod, get_transforms
transform = get_transforms(image_size=256, normalization=InputNormalizationMethod.NONE)
```

然后我们可以按如下方式创建数据集:

```python
folder_dataset_classification_train = FolderDataset(
    normal_dir=dataset_root / "good",
    abnormal_dir=dataset_root / "crack",
    split="train",
    transform=transform,
    task=TaskType.CLASSIFICATION,
)
```

- 制作文件夹数据集

假设我们想使用 make_folder_dataset 从文件夹创建数据集。然后我们可以按如下方式创建数据集:

```python
folder_df = make_folder_dataset(
    normal_dir=dataset_root / "good",
    abnormal_dir=dataset_root / "crack",
    split="train",
)
folder_df.head()
```



##### 2、模型

```python
# 当前anomalib模块支持的异常检测模型：
"""
Cfa,
Cflow,
Csflow,
Dfkde,
Dfm,
Draem,
Dsr,
EfficientAd,
Fastflow,
Ganomaly,
Padim,
Patchcore,
ReverseDistillation,
Rkde,
Stfpm,
Uflow,
WinClip,
"""
model = Fastflow(input_size=(256, 256),
                    backbone='resnet18',
                    pre_trained=True,
                    flow_steps=8,
                    conv3x3_only=False,
                    hidden_ratio=1.0,)
```



##### 3、训练



