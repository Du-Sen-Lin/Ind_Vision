# DL

## 一、基础知识一：

### 输入端：

#### 1、数据增强：

目的：增加训练数据的多样性和数量，从而改善模型的性能。

**图像翻转 **/ **随机裁剪** / **旋转和缩放** / **亮度和对比度调整** / **添加噪声** / **颜色变换**/ **混合数据** / **mosaic** / ......

**mosaic**: 采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接.

**Mixup 正则化**: Mixup 是一种数据增强技术，旨在提升神经网络的泛化能力。它的基本思想是将两个不同样本的输入和标签按照一个随机比例进行线性插值，从而生成一个新的样本。

1. 随机选择另一个样本。

2. 随机选择一个在 [0, 1] 区间内的比例因子 lambda。

3. 将两个样本的输入进行线性插值，即：

   ```python
   new_input = lambda * input1 + (1 - lambda) * input2
   ```

4. 将两个样本的标签进行线性插值，即：

   ```python
   new_label = lambda * label1 + (1 - lambda) * label2
   ```

这样生成的新样本对应着混合了两个原始样本的特征和标签。这种技术有助于使得模型在训练过程中对于输入数据的小扰动具有更好的容忍性，从而提升了模型的泛化能力。

Cutout数据增强：在每个训练样本中，随机选择一个区域，并将该区域的像素值置为零（或者用随机的像素值填充）。

#### 2、归一化：

目的：应用于输入数据以改善模型的训练和性能。归一化的主要目的是将输入数据的特征值进行缩放，使其具有相似的尺度和分布，从而帮助优化算法更快地收敛，减少梯度消失问题，并提高模型的稳定性。

**均值归一化**：均值归一化是通过减去数据的均值来实现的，以使数据的均值接近于零。这通常对图像数据中的像素值或特征向量中的特征进行操作。

**标准差归一化**：标准差归一化是将数据除以其标准差，以确保数据的方差接近于1。这有助于数据的分布更加集中在某个范围内。

**最小-最大缩放**：最小-最大缩放将数据缩放到一个特定的范围，通常是[0, 1]或[-1, 1]。这可以保证数据的所有特征值都在相同的区间内。例如pytorch中的to_tensor 就是将图像 0-255 缩放到 0- 1之间：

```python
from torchvision.transforms.functional import to_tensor
if isinstance(img, np.ndarray):
    img = to_tensor(img)
```

**白化（Whitening）**：白化是一种更高级的归一化技术，它不仅将数据的均值归一化为零，还将数据的协方差矩阵转换为单位矩阵。这有助于减少特征之间的相关性。

**批次归一化（Batch Normalization）**：批次归一化是一种特殊的归一化技术，用于深度神经网络中。它在每个训练批次内对数据进行均值和标准差的归一化，从而稳定了网络的训练过程。通常卷积层之后和非线性激活函数之前应用批量归一化。

**局部响应归一化（Local Response Normalization）**：局部响应归一化是一种用于卷积神经网络（CNN）的归一化技术，用于增强特征的鲁棒性。

#### 3、Mini-batch梯度下降

梯度下降算法的一种变体，它将训练数据集分成小批次（mini-batch）并使用每个小批次来计算梯度和更新模型参数。

```markdown
为何不使⽤batch梯度下降（所有样本计算梯度）？硬件要求⾼；计算时间⻓；不⽅便观察训练效果；
随机梯度下降（每个样本当⼀个⼦集）：部分样本存在噪声，不断震荡；
Mini-Batch梯度下降（将训练集拆分成多个⼦集进⾏梯度下降，⼀个iteration即⼀个⼦集进⾏了
梯度下降，⼀个epoch即所有样本进⾏了⼀次梯度下降）：后⼀个⼦集在前⼀个⼦集的基础上调整优
化，加快训练速度，随时看到训练效果，⽅便算法⼈员及时调整策略。 batch_size:2的次⽅，适合GPU特性，运算更快。
```

流程：

1. **数据集划分**：将训练数据集分成若干个小批次，每个小批次包含一定数量的训练样本。这个数量通常是一个可调参数，称为批次大小（batch size）。
2. **随机化**：通常在每个 epoch（整个训练数据集的一次完整遍历）开始之前，对训练数据进行随机化排列，以确保每个小批次包含来自不同类别和样本的数据，避免模型受到数据排列的影响。
3. **前向传播**：对于每个小批次，将输入数据传递到神经网络中进行前向传播，计算模型的预测输出。
4. **计算损失**：使用模型的预测结果和真实标签来计算损失函数，用于衡量模型的性能。
5. **反向传播**：计算损失函数相对于模型参数的梯度。这是 Mini-batch 梯度下降的关键步骤，通过链式法则逆向传播梯度，然后用于更新模型参数。
6. **参数更新**：根据梯度计算的信息，使用一个优化算法（通常是随机梯度下降或其变种）来更新模型的参数。这将使模型更接近最优参数。
7. **重复迭代**：重复上述步骤，直到达到预定的迭代次数或达到收敛条件（例如，损失函数不再显著减小）。

#### 4、参数（权重**Weights**与偏差**Biases**）随机初始化

参数初始化指的是在训练开始之前为神经网络的权重和偏差（参数）赋予初始值。权重即为卷积核（含有可学习参数的滤波器）。迁移学习就是使用在大数据上训练好的权重和偏差作为初始化参数。

正确的参数随机初始化可以加速训练过程，帮助神经网络更容易地学习到有用的特征表示。错误的初始化可能导致训练过程非常缓慢，甚至无法收敛。

1. **权重初始化**：神经网络中的权重是用于连接不同层之间的参数。在参数随机初始化中，通常会将权重初始化为随机小数值。这有助于打破对称性，以便在训练过程中不同神经元的学习可以发展出不同的特征表示。
2. **偏差初始化**：除了权重之外，神经网络的每个神经元通常还具有一个偏差（bias）参数。这些偏差也需要进行随机初始化，通常初始化为小的随机数值。
3. **初始化分布**：选择随机初始化的分布是一个重要的决策。常见的初始化分布包括均匀分布（通常在一定范围内随机选择值）和正态分布（从均值为零的正态分布中随机选择值）。具体选择哪种分布取决于问题的性质和网络的架构。
4. **缩放因子**：有时，在初始化过程中，会使用一个缩放因子（scale factor）来调整随机初始化的权重。这可以确保初始值的范围适合网络的结构和激活函数。
5. **预训练**：在某些情况下，可以使用预训练的权重，例如从一个在大规模数据上训练过的模型中导入参数，然后对其进行微调。

### 中间层：

#### 1、卷积层

在卷积神经网络中，通常会有多个卷积层，每个卷积层都由多个滤波器组成。这些滤波器在卷积运算过程中可以学习到不同的图像特征，并将这些特征传递给下一层。通过这种方式，神经网络可以逐步将图像的低级特征（例如边缘和角点）组合成高级特征（例如对象的部分或整体），从而实现图像识别任务。

1. **特征提取**：卷积层用于自动提取图像中的特征，这些特征包括边缘、纹理、形状和更高级的视觉概念，如目标或物体的部分。这些特征通常以多个卷积核的形式表示。
2. **局部连接**：卷积层通过局部连接来处理输入数据。每个卷积核只与输入数据的一个小区域（感受野）相连接，而不是与整个输入图像相连接。这种局部连接的方式有助于减少参数数量，提高模型的计算效率。
3. **权重共享**：卷积层中的卷积核在整个输入图像上共享权重。这意味着学习到的特征对于图像的不同区域都是相同的，这种共享有助于模型更好地泛化。
4. **步幅（Stride）**：卷积操作中的步幅控制着卷积核在输入上滑动的距离。较大的步幅会减小输出特征图的大小，减少计算量。较小的步幅可以保留更多的空间信息。
5. **填充（Padding）**：填充是在输入图像的边缘添加额外的像素，以便在卷积操作后保持输出特征图的大小。填充通常分为“有效（valid）”和“相同（same）”两种方式。
6. **激活函数**：卷积层通常在卷积操作后应用激活函数，如ReLU（Rectified Linear Unit），以引入非线性性质，使网络能够学习更复杂的特征表示。
7. **多通道**：卷积层可以处理多通道的输入图像，例如RGB彩色图像具有3个通道。每个通道都有一组卷积核，用于提取不同通道的特征。
8. **多层卷积**：卷积层可以堆叠在一起形成深层卷积神经网络。通过多层卷积操作，网络可以逐渐学习到更高级别的特征，从边缘和纹理到更复杂的物体部分和整个物体。

```python
卷积层：
为什么？全连接层参数量⼤，容易过拟合，计算需求⼤。
padding(填充)? 保持尺⼨⼀样，⽅便建⽴更深的卷积神经⽹络。
公式：n_out = (n_in + 2*p -f )/ s + 1; f-卷积核尺⼨， s-步⻓；
多维卷积操作：如6x6x3⸺3x3x3卷积核（三维）⸺> 4x4x1; 图像处理中常用的是2维卷积核。
多卷积核卷积操作：如6x6x3⸺(3x3x3)x2 卷积核⸺> 4x4x2
4-2、池化层：
为什么？降低⽹络计算量，提⾼⽹络的鲁棒性，保留更重要的特征。
⽅式：Max-Pooling, 如 6x6x1⸺3x3x1,s=2,p=0⸺>2x2x1; Average-Pooling, 如6x6x3⸺
3x3x3⸺>2x2x3;
4-3、经典⽹络：
Lenet-5(1998): 卷积局部连接，减少连接数量和⽹络参数，不对称连接⽅式，学习到不同的特征
表⽰；
Alexnet(2012): same_conv: 使⽤padding, 使得尺⼨不变；Dropout: 减少过拟合。
VGG16(2014): ⽹络更深；更⼩的卷积核3x3,可以学习更复杂的特征且参数更少。
ResNet(2015): ⽹络更深可以提升性能，但会造成梯度爆炸（随着⽹络传播，激活值越来越⼤，
最后变成⽆穷⼤）或者梯度消失（随着⽹络传播，激活值越来越⼩，最后变成⽆穷⼩）；残差⽹络：
采⽤跳跃连接的⽅式，将前⾯的激活值跳跃中间层，直接传递到后⾯的⽹络层中。
Inception(2014): 增加⽹络宽度，Inception模块组成GoogleNet⽹络；不增加计算成本扩展⽹络：使⽤1x1卷积降低通道，降低计算量。1x1卷积：降低维度，减少计算量；增加⽹络层数，提⾼⽹络性能。
```



#### 2、网络归一化

目的：提高网络的训练稳定性和性能。网络归一化的主要目标是规范网络层的输入，使其具有统一的统计特性，有助于更快地收敛，并减轻梯度消失问题。

1. **批次归一化（Batch Normalization，BN）**：批次归一化是一种常见的网络归一化技术，应用于深度神经网络中的卷积层和全连接层。它的主要思想是对每个批次的输入进行归一化，然后通过可学习的参数进行缩放和平移。具体来说，对于每个特征通道，批次归一化计算该通道在一个小批次内的均值和标准差，然后将输入进行线性变换，以使其均值为零，标准差为一，并通过缩放和平移参数还原数据的表示能力。批次归一化有助于克服内部协变量偏移（Internal Covariate Shift）问题，提高训练速度和稳定性。
2. **组归一化（Group Normalization，GN）**：组归一化是一种替代批次归一化的归一化技术，它将输入特征通道分成多个组，然后对每个组内的特征进行归一化。这有助于处理小批次大小的情况，因为批次归一化可能在小批次上效果不佳。组归一化的主要思想是在通道维度上进行归一化，而不是在批次维度上。组归一化通常用于卷积层中。

#### 3、激活函数

目的：在网络的每个神经元上执行非线性操作，以引入非线性性质和增强网络的表达能力。

```markdown
不使⽤激活函数？退化为线性函数。
激活函数（⾮线性函数，分段函数也是⾮线性函数）：解决⾮线性问题。常⻅的激活函数:sigmoid; tanh; ReLU; LeakyReLU; Mish
```

1. **ReLU（Rectified Linear Unit）**：

   - 公式：ReLU(x) = max(0, x)
   - 特点：对于正数输入，输出等于输入；对于负数输入，输出为零。ReLU 是一种常用的激活函数，它具有非线性性质，但可能导致**死神经元问题（在训练过程中某些神经元永远不会激活,  梯度为0，如ReLU 激活函数在负数区域，Sigmoid 和 Tanh 激活函数在输入值较大或较小时会饱和，导致梯度接近于零，从而无法传播有效的信号。这被称为梯度消失问题）**。

   ReLU 具有以下特点：

   - 非线性：ReLU 引入了非线性性质，使得网络可以学习复杂的特征表示。
   - 稀疏性：ReLU 常常会使一些神经元处于非活跃状态，这可以看作是一种特征选择。
   - 快速收敛：ReLU 可以加速网络的收敛速度。
   - 在卷积神经网络中广泛使用，用于提取图像特征。

2. **Sigmoid 函数**：Sigmoid 函数是一个常见的激活函数，它将输入映射到 (0, 1) 范围内。Sigmoid 函数在二元分类问题中经常用作输出层的激活函数，用于将模型输出映射到概率分布。

   - 公式：Sigmoid(x) = 1 / (1 + exp(-x))
   - 特点：Sigmoid 函数将输入映射到 (0, 1) 范围内，通常用于二元分类问题的输出层。

3. **Tanh 函数**：Tanh 函数将输入映射到 (-1, 1) 范围内。它具有零中心化特性，对输入的正负性更敏感，因此在某些网络架构中用于隐藏层的激活函数。

   - 公式：Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
   - 特点：Tanh 函数将输入映射到 (-1, 1) 范围内，具有零中心化性质。

4. **Leaky ReLU**：

   - 公式：Leaky ReLU(x) = x，x >= 0；Leaky ReLU(x) = αx，x < 0，其中 α 是小的正数。
   - 特点：Leaky ReLU 在负数输入上不是完全取零，而是返回一个小的负斜率，有助于解决死神经元问题。

5. **PReLU（Parametric ReLU）**：

   - 公式：PReLU(x) = x，x >= 0；PReLU(x) = αx，x < 0，其中 α 是可学习参数。
   - 特点：PReLU 是 Leaky ReLU 的扩展，它允许每个神经元学习自己的负斜率。

6. **Swish 函数**：

   - 公式：Swish(x) = x * sigmoid(x)
   - 特点：Swish 函数结合了 Sigmoid 和线性函数的特点，具有平滑的非线性性质。

7. **GELU（Gaussian Error Linear Unit）**：

   **GELU（Gaussian Error Linear Unit）**：

   - 公式：GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)))
   - 特点：GELU 是基于高斯误差函数的激活函数，近年来在深度神经网络中得到广泛使用。

8. **Mish**: Mish 函数结合了双曲正切函数（tanh）和 softplus 函数的特点，具有以下一些性质和优点：

   1. **平滑性**：Mish 函数是平滑的，这意味着它在整个输入范围内都有梯度，有助于训练过程的稳定性。
   2. **非线性性**：Mish 是一种非线性激活函数，可以帮助神经网络学习复杂的特征表示。
   3. **渐近性质**：在输入的极端值上，Mish 的值接近于线性，这有助于减轻梯度爆炸问题。
   4. **类似于 ReLU**：在输入接近零的地方，Mish 的形状类似于 ReLU 函数，这意味着在正数输入上，Mish 保留了 ReLU 的优点，例如快速收敛。
   5. 公式：Mish(x) = x * tanh(softplus(x))。其中，softplus(x) = ln(1 + exp(x))。

#### 4、学习率衰减

学习率衰减：初期：lr较⼤，梯度更新快；后期：lr降低，降低震荡幅度，收敛更精确。

1. **学习率（Learning Rate）**：学习率是训练神经网络时用于更新模型参数的重要超参数。它决定了每一步中参数更新的幅度。较高的学习率可以加速训练过程，但可能导致不稳定的收敛，而较低的学习率可以提高收敛的稳定性，但可能需要更多的迭代。
2. **学习率衰减（Learning Rate Decay）**：学习率衰减是在训练过程中逐渐减小学习率的策略。它的目的是在训练开始时使用较大的学习率以加快收敛速度，然后随着训练的进行逐渐减小学习率，以稳定训练并更精确地接近最优解。
3. **衰减方法**：学习率衰减有多种方法，常见的包括：
   - **固定衰减（Fixed Decay）**：在训练的每个固定迭代步骤或每个 epoch 结束时，将学习率乘以一个小的衰减因子。
   - **按时间表衰减（Scheduled Decay）**：在训练的每个时间段或固定迭代步骤后，调整学习率。例如，每隔一定的时间段或固定步骤减小学习率。
   - **性能观察衰减（Performance-based Decay）**：根据模型性能的表现来动态调整学习率。如果模型性能停止提升，可以降低学习率以寻找更好的解。
   - **余弦退火（Cosine Annealing）**：学习率按照余弦函数的形状逐渐衰减，通常与周期性训练循环结合使用。
4. **选择衰减率**：选择适当的衰减率和衰减策略通常需要通过实验来确定。过大的衰减率可能导致模型无法收敛，而过小的衰减率可能使学习率在训练早期仍然很高，导致训练不稳定。
5. **监视性能**：在使用性能观察衰减策略时，通常需要监视验证集上的性能来决定是否降低学习率。如果性能不再提升，可以降低学习率。

### 输出端：

#### 1、损失函数

用于衡量模型预测与真实标签之间差异的函数。损失函数在深度学习中的输出端通常用于训练模型，通过调整模型参数以最小化损失函数，使模型能够更好地适应任务。

1. **交叉熵损失（Cross-Entropy Loss）**：也称为对数损失（Log Loss），是在多分类任务中广泛使用的损失函数。对于每个样本，交叉熵损失衡量了模型对真实标签的预测与实际标签的差异。对于 Softmax 输出层用于多类别分类任务来说，交叉熵损失通常是首选的损失函数。
2. **均方误差损失（Mean Squared Error Loss）**：在回归任务中常用的损失函数，用于衡量模型的连续数值输出与真实数值标签之间的平方差。例如，用于预测图像中物体的坐标或像素值。
3. **对比损失（Contrastive Loss）**：主要用于度量两个样本之间的相似度。它通常在孪生网络（Siamese Network）中用于学习图像匹配和检索任务。
4. **三元损失（Triplet Loss）**：用于度量三个样本之间的相对距离，通常在人脸验证和人脸识别等任务中使用。它有助于确保同一类别的样本之间的距离小于不同类别的样本之间的距离。
5. **Focal Loss**：用于解决类别不平衡问题，尤其在目标检测任务中。它通过调整样本的权重，使模型更加关注难以分类的样本。
6. **Dice Loss**：在图像分割任务中常用的损失函数，用于度量模型预测的掩码与真实掩码之间的相似度。
7. **Huber Loss**：在回归任务中的一种鲁棒损失函数，对异常值具有一定的鲁棒性。

回归损失函数：

- 均⽅差（Mean Squared Error，MSE） 
- 平均绝对误差（Mean Absolute Error Loss，MAE） 

分类损失函数：

- 对数损失函数 LogLoss ⸺逻辑回归就是使⽤对数损失函数来进⾏⼆分类任务 
- 交叉熵损失函数 CrossEntropyLoss ⸺softmax回归使⽤交叉熵损失函数来进⾏多分类任务 

https://blog.csdn.net/happy488127311/article/details/123750000

https://blog.csdn.net/Elaboraty/article/details/107132027

#### 2、Softmax多分类器

```markdown
Sigmoid: 判断⼀个类别是或者不是；
softmax:对于互斥情况，即⼀张图⽚中只有⼀个类别。概率之和为1.
对于不是互斥的情况，如⼀张图⽚属于多个类别，可以使⽤多个sigmod激活函数，这也就是多任
务学习。
```

```markdown
⸺L2范数 向量元素平⽅和的平⽅根: u = torch.tensor([3.0, -4.0]) torch.norm(u)
⸺L1范数 向量元素的绝对值之和: torch.abs(u).sum()
⸺softmax函数：多分类将各个输出节点的输出值范围映射到[0, 1]，并且约束各个输出节点的输出值的和为1的函数。使⽤指数函数e^x. softmax(xi) = e^xi / ( e ^x0 + e^x1 + ... e^xn)
⸺softmax loss : softmax是激活函数，交叉熵是损失函数，softmax loss是使⽤了softmax funciton的交叉熵损失。 如5分类问题：真实标签[0, 0 , 0, 1, 0], softmax 输出：[0.1, 0.15, 0.05, 0.6, 0.1], softmax loss 公式：-Elogy^i * logp^i = -log(0.6)
https://www.cnblogs.com/Joejwu/p/Joejwu_blog210618.html
https://zhuanlan.zhihu.com/p/139696588
https://blog.csdn.net/happy488127311/article/details/123750000
```



